"""
è—¥ç‰©åœ–ç‰‡è¾¨è­˜æ¨¡çµ„
æ”¯æ´å¤šç¨®è¾¨è­˜æ–¹æ³•ï¼šç‰¹å¾µæ¯”å°ã€OCR æ–‡å­—è¾¨è­˜
"""

import cv2
import numpy as np
from pathlib import Path
from typing import List, Dict, Optional, Tuple, Any
import sqlite3
import threading


class DrugImageRecognizer:
    """è—¥ç‰©åœ–ç‰‡è¾¨è­˜å™¨ï¼ˆç‰¹å¾µæ¯”å°æ–¹æ³•ï¼‰"""

    def __init__(
        self, db_path: str = "drug_recognition.db", photo_dir: str = "medicine_photos"
    ):
        self.db_path = db_path
        self.photo_dir = Path(photo_dir)
        self._image_records: List[Dict[str, str]] = []
        self._metadata_loaded = False
        self._feature_cache: Dict[str, Tuple[np.ndarray, float, np.ndarray]] = (
            {}
        )  # å¢åŠ  LBP
        self._orb_cache: Dict[str, Optional[np.ndarray]] = {}
        self._features_loaded = False
        self._load_lock = threading.Lock()
        self._computed_count = 0
        self._orb = cv2.ORB_create(nfeatures=500)
        self._load_thread: Optional[threading.Thread] = threading.Thread(
            target=self._load_database_features, daemon=True
        )
        self._load_thread.start()

    def _load_image_metadata(self) -> None:
        """è¼‰å…¥æ‰€æœ‰è—¥ç‰©åœ–ç‰‡çš„è³‡æ–™åˆ—ã€‚"""
        if self._metadata_loaded:
            return

        with self._load_lock:
            if self._metadata_loaded:
                return

            conn = None
            try:
                conn = sqlite3.connect(self.db_path)
                cursor = conn.cursor()
                cursor.execute(
                    """
                    SELECT DISTINCT d.id, d.chinese_name, d.english_name, d.license_number,
                           d.shape, d.color, d.special_dosage_form, i.image_filename
                    FROM drugs d
                    INNER JOIN drug_images i ON d.id = i.drug_id
                """
                )
                rows = cursor.fetchall()
                self._image_records = [
                    {
                        "drug_id": row[0],
                        "chinese_name": row[1],
                        "english_name": row[2],
                        "license_number": row[3],
                        "shape": row[4],
                        "color": row[5],
                        "special_dosage_form": row[6],
                        "image_filename": row[7],
                    }
                    for row in rows
                ]
                self._metadata_loaded = True
            except Exception as exc:
                print(f"âš ï¸ ç„¡æ³•è¼‰å…¥è³‡æ–™åº«åœ–ç‰‡æ¸…å–®: {exc}")
                self._image_records = []
            finally:
                try:
                    if conn:
                        conn.close()
                except Exception:
                    pass

    def _get_or_compute_features(
        self, record: Dict[str, str]
    ) -> Optional[Tuple[np.ndarray, float, np.ndarray]]:
        """å–å¾—æˆ–è¨ˆç®—æŒ‡å®šåœ–ç‰‡çš„ç‰¹å¾µ (é¡è‰²ã€å½¢ç‹€ã€LBPç´‹ç†)ã€‚"""

        filename = record["image_filename"]
        with self._load_lock:
            cached = self._feature_cache.get(filename)
        if cached is not None:
            return cached

        image_path = self.photo_dir / filename
        if not image_path.exists():
            return None

        db_img = self.preprocess_image(str(image_path), apply_denoise=False)
        if db_img is None:
            return None

        db_hist = self.extract_color_histogram(db_img)
        db_shape = self.extract_shape_features(db_img)
        db_lbp = self.extract_lbp_features(db_img)
        features = (db_hist, db_shape.get("circularity", 0.0), db_lbp)

        with self._load_lock:
            self._feature_cache[filename] = features
            self._computed_count = len(self._feature_cache)

        total = len(self._image_records)
        if total and self._computed_count % 200 == 0:
            print(f"ğŸ“¸ å·²è¨ˆç®— {self._computed_count}/{total} å¼µè—¥å“åœ–ç‰‡ç‰¹å¾µ")

        return features

    def _get_or_compute_orb(
        self, filename: str, image_path: Path
    ) -> Optional[np.ndarray]:
        """å»¶é²è¨ˆç®—æŒ‡å®šåœ–ç‰‡çš„ ORB æè¿°å­ä¸¦å¿«å–ã€‚"""

        with self._load_lock:
            if filename in self._orb_cache:
                return self._orb_cache[filename]

        if not image_path.exists():
            with self._load_lock:
                self._orb_cache[filename] = None
            return None

        db_img = self.preprocess_image(str(image_path), apply_denoise=False)
        if db_img is None:
            with self._load_lock:
                self._orb_cache[filename] = None
            return None

        descriptors = self.extract_orb_descriptors(db_img)

        with self._load_lock:
            self._orb_cache[filename] = descriptors

        return descriptors

    def _match_filters(
        self,
        record: Dict[str, str],
        filter_shape: Optional[str],
        filter_color: Optional[str],
    ) -> bool:
        """
        æª¢æŸ¥è—¥ç‰©è¨˜éŒ„æ˜¯å¦ç¬¦åˆå½¢ç‹€å’Œé¡è‰²ç¯©é¸æ¢ä»¶

        Args:
            record: è—¥ç‰©è¨˜éŒ„å­—å…¸
            filter_shape: ç¯©é¸å½¢ç‹€
            filter_color: ç¯©é¸é¡è‰²

        Returns:
            True å¦‚æœç¬¦åˆæ‰€æœ‰æŒ‡å®šçš„ç¯©é¸æ¢ä»¶ï¼Œå¦å‰‡ False
        """
        # æª¢æŸ¥å½¢ç‹€
        if filter_shape:
            drug_shape = record.get("shape", "")
            if not drug_shape or filter_shape not in drug_shape:
                return False

        # æª¢æŸ¥é¡è‰²
        if filter_color:
            drug_color = record.get("color", "")
            if not drug_color or filter_color not in drug_color:
                return False

        return True

    def _infer_color_labels(self, image: np.ndarray) -> List[str]:
        """
        ç”±åœ–ç‰‡æ¨ä¼°é¡è‰²æ¨™ç±¤ï¼ˆä¸­æ–‡ï¼‰ï¼Œå›å‚³å€™é¸æ¨™ç±¤åˆ—è¡¨ï¼Œç”¨æ–¼ç¸®å°æ¯”å°ç¯„åœã€‚

        å¯èƒ½å›å‚³ï¼š['ç™½', 'ç™½è‰²']ã€['ç´…', 'ç´…è‰²']ã€['é»ƒ', 'é»ƒè‰²']ã€['ç¶ ', 'ç¶ è‰²']ã€['è—', 'è—è‰²']ã€
                 ['ç´«', 'ç´«è‰²']ã€['æ©™', 'æ©˜', 'æ©™è‰²', 'æ©˜è‰²']ã€['é»‘', 'é»‘è‰²']ã€['ç°', 'ç°è‰²']ã€['æ£•', 'å’–å•¡', 'æ£•è‰²', 'å’–å•¡è‰²']
        """
        try:
            hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
            h = hsv[:, :, 0].astype(np.float32)
            s = hsv[:, :, 1].astype(np.float32)
            v = hsv[:, :, 2].astype(np.float32)

            mean_s = float(np.mean(s))
            mean_v = float(np.mean(v))

            if mean_s < 30:  # ä½é£½å’Œï¼šé»‘/ç™½/ç°
                if mean_v > 180:
                    return ["ç™½", "ç™½è‰²"]
                elif mean_v < 60:
                    return ["é»‘", "é»‘è‰²"]
                else:
                    return ["ç°", "ç°è‰²"]

            # ä»¥ Hue å¹³å‡ä¼°è‰²èª¿
            mean_h = float(np.mean(h))  # 0~180
            if mean_h <= 10 or mean_h >= 160:
                return ["ç´…", "ç´…è‰²"]
            if 11 <= mean_h <= 25:
                return ["æ©™", "æ©˜", "æ©™è‰²", "æ©˜è‰²"]
            if 26 <= mean_h <= 34:
                return ["é»ƒ", "é»ƒè‰²"]
            if 35 <= mean_h <= 85:
                return ["ç¶ ", "ç¶ è‰²"]
            if 86 <= mean_h <= 125:
                return ["è—", "è—è‰²"]
            if 126 <= mean_h <= 159:
                return ["ç´«", "ç´«è‰²"]

            # å…¶ä»–è‰²èª¿è¦–ç‚ºæ£•/å’–å•¡
            return ["æ£•", "å’–å•¡", "æ£•è‰²", "å’–å•¡è‰²"]
        except Exception:
            return []

    def _load_database_features(self) -> None:
        """èƒŒæ™¯è¼‰å…¥åœ–ç‰‡æ¸…å–®ä¸¦é å…ˆè¨ˆç®—å°‘é‡ç‰¹å¾µã€‚"""

        self._load_image_metadata()

        total = len(self._image_records)
        preload_cap = min(50, total)
        loaded = 0

        if preload_cap:
            for record in self._image_records[:preload_cap]:
                if self._get_or_compute_features(record) is not None:
                    loaded += 1

        with self._load_lock:
            self._features_loaded = loaded >= total and total > 0

        if total:
            print(
                f"âœ… å·²é å…ˆè¼‰å…¥ {loaded}/{total} ç­†è—¥å“åœ–ç‰‡ç‰¹å¾µï¼ˆå…¶é¤˜å°‡æ–¼æŸ¥è©¢æ™‚å‹•æ…‹è¨ˆç®—ï¼‰"
            )
        else:
            print("âš ï¸ æœªåœ¨è³‡æ–™åº«ä¸­æ‰¾åˆ°å¯ç”¨çš„è—¥å“åœ–ç‰‡")

    def reload_feature_cache(self, async_load: bool = False) -> None:
        """é‡æ–°æ•´ç†å¿«å–ï¼Œæ”¯æ´èƒŒæ™¯è¼‰å…¥ã€‚"""

        def _reload():
            with self._load_lock:
                self._features_loaded = False
                self._feature_cache.clear()
                self._metadata_loaded = False
                self._computed_count = 0
            self._load_database_features()

        if async_load:
            threading.Thread(target=_reload, daemon=True).start()
        else:
            _reload()

    def preprocess_image(
        self, image_path: str, apply_denoise: bool = True
    ) -> Optional[np.ndarray]:
        """
        é è™•ç†åœ–ç‰‡ï¼šèª¿æ•´å¤§å°ã€å»å™ª

        Args:
            image_path: åœ–ç‰‡è·¯å¾‘

        Returns:
            è™•ç†å¾Œçš„åœ–ç‰‡é™£åˆ—ï¼Œå¤±æ•—è¿”å› None
        """
        try:
            # ä½¿ç”¨ cv2.imdecode è™•ç†ä¸­æ–‡è·¯å¾‘
            import numpy as np

            # è®€å–åœ–ç‰‡ï¼ˆæ”¯æ´ä¸­æ–‡è·¯å¾‘ï¼‰
            with open(image_path, "rb") as f:
                image_data = f.read()
            nparr = np.frombuffer(image_data, np.uint8)
            img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)

            if img is None:
                return None

            # èª¿æ•´å¤§å°ï¼ˆæ¨™æº–åŒ–ï¼‰
            img = cv2.resize(img, (300, 300))

            # é™å™ªé‹ç®—æˆæœ¬é«˜ï¼Œåƒ…é‡å°ä¸Šå‚³åœ–ç‰‡åŸ·è¡Œ
            if apply_denoise:
                img = cv2.fastNlMeansDenoisingColored(img, None, 10, 10, 7, 21)

            return img
        except Exception as e:
            print(f"åœ–ç‰‡é è™•ç†å¤±æ•—: {e}")
            return None

    def extract_color_histogram(self, image: np.ndarray) -> np.ndarray:
        """
        æå–é¡è‰²ç›´æ–¹åœ–ç‰¹å¾µ

        Args:
            image: åœ–ç‰‡é™£åˆ—

        Returns:
            é¡è‰²ç›´æ–¹åœ–ç‰¹å¾µå‘é‡
        """
        # è½‰æ›åˆ° HSV è‰²å½©ç©ºé–“
        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)

        # è¨ˆç®—ç›´æ–¹åœ–
        hist = cv2.calcHist([hsv], [0, 1, 2], None, [8, 8, 8], [0, 180, 0, 256, 0, 256])

        # æ­£è¦åŒ–
        hist = cv2.normalize(hist, hist).flatten()

        return hist

    def extract_shape_features(self, image: np.ndarray) -> Dict[str, float]:
        """
        æå–å½¢ç‹€ç‰¹å¾µ

        Args:
            image: åœ–ç‰‡é™£åˆ—

        Returns:
            å½¢ç‹€ç‰¹å¾µå­—å…¸
        """
        # è½‰ç‚ºç°éš
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

        # äºŒå€¼åŒ–
        _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)

        # å°‹æ‰¾è¼ªå»“
        contours, _ = cv2.findContours(
            binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE
        )

        if not contours:
            return {"area": 0, "perimeter": 0, "circularity": 0}

        # å–æœ€å¤§è¼ªå»“
        main_contour = max(contours, key=cv2.contourArea)

        # è¨ˆç®—ç‰¹å¾µ
        area = cv2.contourArea(main_contour)
        perimeter = cv2.arcLength(main_contour, True)
        circularity = 4 * np.pi * area / (perimeter**2) if perimeter > 0 else 0

        return {"area": area, "perimeter": perimeter, "circularity": circularity}

    def extract_orb_descriptors(self, image: np.ndarray) -> Optional[np.ndarray]:
        """æå– ORB ç‰¹å¾µæè¿°å­ä»¥è¾¨è­˜è—¥éŒ åˆ»å°ã€‚"""

        try:
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            keypoints, descriptors = self._orb.detectAndCompute(gray, None)
            if descriptors is None or len(descriptors) == 0:
                return None
            return descriptors
        except Exception as exc:
            print(f"ORB ç‰¹å¾µæå–å¤±æ•—: {exc}")
            return None

    def extract_lbp_features(
        self, image: np.ndarray, radius: int = 1, n_points: int = 8
    ) -> np.ndarray:
        """
        æå– LBP (Local Binary Pattern) ç´‹ç†ç‰¹å¾µï¼ˆé«˜é€Ÿå‘é‡åŒ–ç‰ˆæœ¬ï¼‰

        èªªæ˜ï¼š
        - å°‡ç°éšåœ–ç¸®æ”¾è‡³ 128x128ï¼Œä½¿ç”¨ 8 é„°åŸŸã€åŠå¾‘ 1 çš„ç¶“å…¸ LBPã€‚
        - ä»¥ numpy ä½å…ƒé‹ç®—è¨ˆç®—ï¼Œä¸ä½¿ç”¨å·¢ç‹€ Python è¿´åœˆï¼Œå¤§å¹…é™ä½å»¶é²ã€‚

        Args:
            image: åœ–ç‰‡é™£åˆ—
            radius: LBP åŠå¾‘ï¼ˆåƒ…æ”¯æ´ 1ï¼Œç”¨æ–¼å¿«é€Ÿé‹ç®—ï¼‰
            n_points: é„°åŸŸé»æ•¸ï¼ˆåƒ…æ”¯æ´ 8ï¼‰

        Returns:
            é•·åº¦ 256 çš„ LBP ç›´æ–¹åœ–ï¼ˆå·²æ­£è¦åŒ–ï¼‰
        """
        try:
            # è½‰ç‚ºç°éšä¸¦ç¸®å°å°ºå¯¸ä»¥é™ä½é‹ç®—é‡
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            gray = cv2.resize(gray, (128, 128), interpolation=cv2.INTER_AREA)
            gray = cv2.GaussianBlur(gray, (3, 3), 0)

            if radius != 1 or n_points != 8:
                # ç‚ºä¿æŒé€Ÿåº¦èˆ‡ç©©å®šæ€§ï¼Œæš«æ™‚åƒ…æ”¯æ´ (radius=1, n_points=8)
                radius = 1
                n_points = 8

            # å…§éƒ¨å€åŸŸï¼ˆé¿å…é‚Šç•Œï¼‰
            c = gray[1:-1, 1:-1]
            codes = np.zeros_like(c, dtype=np.uint8)

            # 8 å€‹é„°å±…ä½ç§»ï¼ˆé †æ™‚é‡ï¼‰
            neighbors = [
                gray[0:-2, 0:-2],  # (-1,-1)
                gray[0:-2, 1:-1],  # (-1, 0)
                gray[0:-2, 2:],  # (-1,+1)
                gray[1:-1, 2:],  # ( 0,+1)
                gray[2:, 2:],  # (+1,+1)
                gray[2:, 1:-1],  # (+1, 0)
                gray[2:, 0:-2],  # (+1,-1)
                gray[1:-1, 0:-2],  # ( 0,-1)
            ]

            for bit, n in enumerate(neighbors):
                codes |= (n >= c).astype(np.uint8) << bit

            # è¨ˆç®—ç›´æ–¹åœ–ä¸¦æ­£è¦åŒ–
            hist, _ = np.histogram(codes.ravel(), bins=256, range=(0, 256))
            hist = hist.astype(np.float32)
            s = hist.sum()
            if s > 0:
                hist /= s

            return hist

        except Exception as e:
            print(f"LBP ç‰¹å¾µæå–å¤±æ•—: {e}")
            return np.zeros(256, dtype=np.float32)

    def extract_mark_features(self, mark_text: str) -> str:
        """
        æå–ä¸¦æ¨™æº–åŒ–åˆ»ç—•ç‰¹å¾µæ–‡å­—

        Args:
            mark_text: è—¥ç‰©åˆ»ç—•æè¿°æ–‡å­—

        Returns:
            æ¨™æº–åŒ–çš„åˆ»ç—•ç‰¹å¾µå­—ä¸²
        """
        if not mark_text or mark_text == "ç„¡" or mark_text == "None":
            return ""

        # ç§»é™¤ç©ºç™½å’Œæ¨™é»ç¬¦è™Ÿ,è½‰ç‚ºå¤§å¯«
        import re

        mark_text = mark_text.upper()
        mark_text = re.sub(r"[^\w\s]", "", mark_text)
        mark_text = re.sub(r"\s+", "", mark_text)

        return mark_text

    def calculate_mark_similarity(self, mark1: str, mark2: str) -> float:
        """
        è¨ˆç®—å…©å€‹åˆ»ç—•æè¿°çš„ç›¸ä¼¼åº¦

        Args:
            mark1: ç¬¬ä¸€å€‹åˆ»ç—•æè¿°
            mark2: ç¬¬äºŒå€‹åˆ»ç—•æè¿°

        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•¸ (0-1)
        """
        m1 = self.extract_mark_features(mark1)
        m2 = self.extract_mark_features(mark2)

        if not m1 or not m2:
            return 0.0

        if m1 == m2:
            return 1.0

        # ä½¿ç”¨åºåˆ—åŒ¹é…è¨ˆç®—ç›¸ä¼¼åº¦
        from difflib import SequenceMatcher

        matcher = SequenceMatcher(None, m1, m2)
        return matcher.ratio()

    def calculate_similarity(self, hist1: np.ndarray, hist2: np.ndarray) -> float:
        """
        è¨ˆç®—å…©å€‹ç›´æ–¹åœ–çš„ç›¸ä¼¼åº¦ï¼ˆä½¿ç”¨ç›¸é—œæ€§ï¼‰

        Args:
            hist1: ç¬¬ä¸€å€‹ç›´æ–¹åœ–
            hist2: ç¬¬äºŒå€‹ç›´æ–¹åœ–

        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•¸ (0-1)
        """
        # ä½¿ç”¨ç›¸é—œæ€§æ–¹æ³•
        similarity = cv2.compareHist(hist1, hist2, cv2.HISTCMP_CORREL)

        # è½‰æ›åˆ° 0-1 ç¯„åœ
        return max(0, similarity)

    def calculate_orb_similarity(
        self, descriptors1: Optional[np.ndarray], descriptors2: Optional[np.ndarray]
    ) -> float:
        """è¨ˆç®— ORB æè¿°å­çš„ç›¸ä¼¼åº¦ï¼Œå€¼åŸŸ 0-1ã€‚"""

        if descriptors1 is None or descriptors2 is None:
            return 0.0

        try:
            matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=False)
            matches = matcher.knnMatch(descriptors1, descriptors2, k=2)
        except cv2.error as exc:
            print(f"ORB æ¯”å°å¤±æ•—: {exc}")
            return 0.0

        good_matches = 0
        for pair in matches:
            if len(pair) < 2:
                continue
            m, n = pair
            if m.distance < 0.75 * n.distance:
                good_matches += 1

        max_possible = min(len(descriptors1), len(descriptors2))
        if max_possible == 0:
            return 0.0

        return good_matches / max_possible

    def calculate_lbp_similarity(self, lbp1: np.ndarray, lbp2: np.ndarray) -> float:
        """
        è¨ˆç®—å…©å€‹ LBP ç›´æ–¹åœ–çš„ç›¸ä¼¼åº¦ï¼ˆä½¿ç”¨å¡æ–¹è·é›¢ï¼‰

        Args:
            lbp1: ç¬¬ä¸€å€‹ LBP ç›´æ–¹åœ–
            lbp2: ç¬¬äºŒå€‹ LBP ç›´æ–¹åœ–

        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•¸ (0-1)
        """
        try:
            # ä½¿ç”¨å¡æ–¹è·é›¢
            chi_square = 0.0
            for i in range(len(lbp1)):
                if lbp1[i] + lbp2[i] > 0:
                    chi_square += ((lbp1[i] - lbp2[i]) ** 2) / (lbp1[i] + lbp2[i])

            # è½‰æ›ç‚ºç›¸ä¼¼åº¦ (è·é›¢è¶Šå°,ç›¸ä¼¼åº¦è¶Šé«˜)
            # ä½¿ç”¨æŒ‡æ•¸å‡½æ•¸å°‡è·é›¢è½‰æ›ç‚º 0-1 ç¯„åœçš„ç›¸ä¼¼åº¦
            similarity = np.exp(-chi_square / 2)

            return float(max(0.0, min(1.0, similarity)))

        except Exception as e:
            print(f"LBP ç›¸ä¼¼åº¦è¨ˆç®—å¤±æ•—: {e}")
            return 0.0

    def recognize_drug(
        self,
        uploaded_image_path: str,
        top_k: int = 5,
        filter_shape: Optional[str] = None,
        filter_color: Optional[str] = None,
        hooks: Optional[Dict[str, Any]] = None,
    ) -> List[Dict]:
        """
        è¾¨è­˜ä¸Šå‚³çš„è—¥ç‰©åœ–ç‰‡

        Args:
            uploaded_image_path: ä¸Šå‚³åœ–ç‰‡çš„è·¯å¾‘
            top_k: è¿”å›å‰ K å€‹æœ€ç›¸ä¼¼çš„çµæœ
            filter_shape: ç¯©é¸å½¢ç‹€ (é¸å¡«)
            filter_color: ç¯©é¸é¡è‰² (é¸å¡«)

        Returns:
            è¾¨è­˜çµæœåˆ—è¡¨ï¼Œæ¯é …åŒ…å«è—¥ç‰©è³‡è¨Šå’Œç›¸ä¼¼åº¦
        """
        import time

        t0 = time.time()

        # hooks
        on_progress = None
        is_cancelled = None
        if isinstance(hooks, dict):
            on_progress = hooks.get("on_progress")
            is_cancelled = hooks.get("is_cancelled")
            if not callable(on_progress):
                on_progress = None
            if not callable(is_cancelled):
                is_cancelled = None

        # é è™•ç†ä¸Šå‚³çš„åœ–ç‰‡
        uploaded_img = self.preprocess_image(uploaded_image_path)
        if uploaded_img is None:
            return []

        # æå–ä¸Šå‚³åœ–ç‰‡çš„ç‰¹å¾µ
        uploaded_hist = self.extract_color_histogram(uploaded_img)
        uploaded_shape = self.extract_shape_features(uploaded_img)
        uploaded_orb = self.extract_orb_descriptors(uploaded_img)
        uploaded_lbp = self.extract_lbp_features(uploaded_img)

        self._load_image_metadata()

        if not self._image_records:
            return []

        # é å…ˆéæ¿¾ç¬¦åˆå½¢ç‹€/é¡è‰²æ¢ä»¶çš„è—¥ç‰©è¨˜éŒ„
        filtered_records = self._image_records
        if filter_shape or filter_color:
            filtered_records = [
                record
                for record in self._image_records
                if self._match_filters(record, filter_shape, filter_color)
            ]
            print(
                f"ğŸ“‹ å¥—ç”¨ç¯©é¸æ¢ä»¶å¾Œï¼Œå‰©é¤˜ {len(filtered_records)}/{len(self._image_records)} ç­†è—¥ç‰©"
            )
        else:
            # æœªæŒ‡å®šç¯©é¸æ™‚ï¼Œä¾æ“šåœ–ç‰‡è‡ªå‹•æ¨ä¼°é¡è‰²ï¼Œç¸®å°æœå°‹ç©ºé–“
            auto_colors = self._infer_color_labels(uploaded_img)
            if auto_colors:
                filtered_records = [
                    r
                    for r in self._image_records
                    if any(lbl in (r.get("color") or "") for lbl in auto_colors)
                ]
                print(
                    f"ğŸ¯ è‡ªå‹•æ¨ä¼°é¡è‰² {auto_colors}ï¼Œå€™é¸ç¸®å°ç‚º {len(filtered_records)}/{len(self._image_records)} ç­†"
                )

        if not filtered_records:
            print("âš ï¸  æ²’æœ‰ç¬¦åˆç¯©é¸æ¢ä»¶çš„è—¥ç‰©")
            return []

        # ç²å–æ‰€æœ‰æœ‰åœ–ç‰‡çš„è—¥ç‰©
        results = []
        total_candidates = len(filtered_records)
        if on_progress:
            try:
                on_progress(0, total_candidates)
            except Exception:
                pass

        for idx, record in enumerate(filtered_records, start=1):
            if is_cancelled and is_cancelled():
                print("ğŸ›‘ æ”¶åˆ°å–æ¶ˆä¿¡è™Ÿï¼Œæå‰çµæŸæ¯”å°")
                break
            features = self._get_or_compute_features(record)
            if features is None:
                continue

            db_hist, db_circularity, db_lbp = features

            # è¨ˆç®—ç›¸ä¼¼åº¦
            color_similarity = self.calculate_similarity(uploaded_hist, db_hist)

            # å½¢ç‹€ç›¸ä¼¼åº¦ï¼ˆç°¡å–®æ¯”è¼ƒåœ“åº¦ï¼‰
            shape_similarity = 1 - abs(uploaded_shape["circularity"] - db_circularity)
            shape_similarity = max(0.0, shape_similarity)

            # LBP ç´‹ç†ç›¸ä¼¼åº¦
            lbp_similarity = self.calculate_lbp_similarity(uploaded_lbp, db_lbp)

            # ORB åˆ»å°ç›¸ä¼¼åº¦ï¼ˆå»¶é²è¨ˆç®—ï¼‰
            orb_similarity = 0.0
            if uploaded_orb is not None and color_similarity >= 0.3:
                image_path = self.photo_dir / record["image_filename"]
                db_orb = self._get_or_compute_orb(record["image_filename"], image_path)
                orb_similarity = self.calculate_orb_similarity(uploaded_orb, db_orb)

            # åˆ»ç—•æ–‡å­—ç›¸ä¼¼åº¦
            mark_similarity = 0.0
            if record.get("mark"):
                # é€™è£¡æš«æ™‚ä½¿ç”¨ 0ï¼Œå› ç‚ºä¸Šå‚³åœ–ç‰‡æ²’æœ‰åˆ»ç—•æ–‡å­—è³‡è¨Š
                # å¦‚æœæœªä¾†åŠ å…¥ OCR è­˜åˆ¥åˆ»ç—•æ–‡å­—ï¼Œå¯ä»¥åœ¨é€™è£¡æ¯”å°
                mark_similarity = 0.0

            # ç¶œåˆç›¸ä¼¼åº¦ï¼ˆèª¿æ•´æ¬Šé‡ï¼‰
            # é¡è‰² 0.25, å½¢ç‹€ 0.15, LBPç´‹ç† 0.30, ORBåˆ»å° 0.30
            overall_similarity = (
                0.25 * color_similarity
                + 0.15 * shape_similarity
                + 0.30 * lbp_similarity
                + 0.30 * orb_similarity
            )

            results.append(
                {
                    **record,
                    "similarity": float(overall_similarity),
                    "similarity_percent": f"{overall_similarity * 100:.1f}%",
                    "details": {
                        "color": f"{color_similarity * 100:.1f}%",
                        "shape": f"{shape_similarity * 100:.1f}%",
                        "texture": f"{lbp_similarity * 100:.1f}%",
                        "imprint": f"{orb_similarity * 100:.1f}%",
                    },
                }
            )

            # é¿å…å–®æ¬¡è«‹æ±‚æ²’æœ‰å›æ‡‰å¤ªä¹…ï¼Œå°å¤§å‹è³‡æ–™é›†æ¯è™•ç† 200 ç­†å°±æ‰“å°ä¸€æ¬¡é€²åº¦
            if idx % 200 == 0:
                elapsed = time.time() - t0
                print(f"â±ï¸ å·²æ¯”å° {idx} ç­†ï¼Œè€—æ™‚ {elapsed:.1f}s")
            if on_progress:
                try:
                    on_progress(idx, total_candidates)
                except Exception:
                    pass

        # æŒ‰ç›¸ä¼¼åº¦æ’åºä¸¦è¿”å›å‰ K å€‹
        results.sort(key=lambda x: x["similarity"], reverse=True)

        elapsed = time.time() - t0
        print(
            f"âœ… æ¯”å°å®Œæˆï¼šå€™é¸ {len(filtered_records)} â†’ å–å‰ {top_k}ï¼Œç¸½è€—æ™‚ {elapsed:.2f}s"
        )
        if on_progress:
            try:
                on_progress(total_candidates, total_candidates)
            except Exception:
                pass

        return results[:top_k]

    def recognize_prescription(self, uploaded_image_path: str) -> Dict:
        """
        è¾¨è­˜è—¥å–®ï¼ˆåŒ…å«å¤šå€‹è—¥ç‰©çš„åœ–ç‰‡ï¼‰

        Args:
            uploaded_image_path: ä¸Šå‚³çš„è—¥å–®åœ–ç‰‡è·¯å¾‘

        Returns:
            è¾¨è­˜çµæœï¼ŒåŒ…å«æª¢æ¸¬åˆ°çš„å¤šå€‹è—¥ç‰©
        """
        # è®€å–åœ–ç‰‡ï¼ˆæ”¯æ´ä¸­æ–‡è·¯å¾‘ï¼‰
        try:
            with open(uploaded_image_path, "rb") as f:
                image_data = f.read()
            nparr = np.frombuffer(image_data, np.uint8)
            img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        except Exception as e:
            return {"success": False, "message": f"ç„¡æ³•è®€å–åœ–ç‰‡: {e}"}

        if img is None:
            return {"success": False, "message": "ç„¡æ³•è®€å–åœ–ç‰‡"}

        # è½‰ç‚ºç°éš
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

        # ä½¿ç”¨è‡ªé©æ‡‰é–¾å€¼
        binary = cv2.adaptiveThreshold(
            gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 11, 2
        )

        # å°‹æ‰¾è¼ªå»“ï¼ˆå¯èƒ½çš„è—¥ç‰©å€åŸŸï¼‰
        contours, _ = cv2.findContours(
            binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE
        )

        # éæ¿¾å¤ªå°çš„è¼ªå»“
        min_area = 1000
        drug_regions = [cnt for cnt in contours if cv2.contourArea(cnt) > min_area]

        # å°æ¯å€‹å€åŸŸé€²è¡Œè¾¨è­˜
        detected_drugs = []

        for i, contour in enumerate(drug_regions[:10]):  # æœ€å¤šè™•ç† 10 å€‹å€åŸŸ
            # ç²å–é‚Šç•Œæ¡†
            x, y, w, h = cv2.boundingRect(contour)

            # è£å‰ªè—¥ç‰©å€åŸŸ
            drug_roi = img[y : y + h, x : x + w]

            # å„²å­˜è‡¨æ™‚åœ–ç‰‡
            temp_path = f"temp_drug_{i}.jpg"
            cv2.imwrite(temp_path, drug_roi)

            # è¾¨è­˜è©²å€åŸŸ
            recognition_result = self.recognize_drug(temp_path, top_k=1)

            if recognition_result:
                detected_drugs.append(
                    {
                        "region": i + 1,
                        "position": {
                            "x": int(x),
                            "y": int(y),
                            "w": int(w),
                            "h": int(h),
                        },
                        "drug": recognition_result[0],
                    }
                )

            # æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
            Path(temp_path).unlink(missing_ok=True)

        return {
            "success": True,
            "total_detected": len(detected_drugs),
            "drugs": detected_drugs,
        }


def test_recognition():
    """æ¸¬è©¦è¾¨è­˜åŠŸèƒ½"""
    recognizer = DrugImageRecognizer()

    # æ¸¬è©¦å–®è—¥ç‰©è¾¨è­˜
    test_image = "test_drug.jpg"  # æ›¿æ›ç‚ºå¯¦éš›æ¸¬è©¦åœ–ç‰‡è·¯å¾‘

    if Path(test_image).exists():
        print("é–‹å§‹è¾¨è­˜...")
        results = recognizer.recognize_drug(test_image, top_k=3)

        print(f"\næ‰¾åˆ° {len(results)} å€‹åŒ¹é…çµæœï¼š")
        for i, result in enumerate(results, 1):
            print(f"\nç¬¬ {i} åï¼š")
            print(f"  è—¥ç‰©åç¨±ï¼š{result['chinese_name']} ({result['english_name']})")
            print(f"  è¨±å¯è­‰å­—è™Ÿï¼š{result['license_number']}")
            print(f"  ç›¸ä¼¼åº¦ï¼š{result['similarity_percent']}")
    else:
        print(f"æ¸¬è©¦åœ–ç‰‡ä¸å­˜åœ¨: {test_image}")


def detect_image_type(image_path: str) -> str:
    """
    è‡ªå‹•åˆ¤æ–·åœ–ç‰‡é¡å‹

    Args:
        image_path: åœ–ç‰‡è·¯å¾‘

    Returns:
        'text': åŒ…å«å¤§é‡æ–‡å­—ï¼ˆè—¥å–®/è—¥è¢‹ï¼‰
        'object': å–®ä¸€ç‰©é«”ï¼ˆè—¥ç‰©ç…§ç‰‡ï¼‰
        'mixed': æ··åˆæˆ–ä¸ç¢ºå®š
    """
    try:
        # è®€å–åœ–ç‰‡
        with open(image_path, "rb") as f:
            image_data = f.read()
        nparr = np.frombuffer(image_data, np.uint8)
        img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)

        if img is None:
            return "mixed"

        # è½‰ç°éš
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

        # é‚Šç·£æª¢æ¸¬
        edges = cv2.Canny(gray, 50, 150)

        # è¨ˆç®—é‚Šç·£å¯†åº¦
        edge_density = np.sum(edges > 0) / edges.size

        # è¼ªå»“æª¢æ¸¬
        contours, _ = cv2.findContours(
            edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE
        )

        # æ–‡å­—å€åŸŸé€šå¸¸æœ‰å¾ˆå¤šå°è¼ªå»“
        small_contours = sum(1 for c in contours if cv2.contourArea(c) < 500)

        # åˆ¤æ–·é‚è¼¯
        if edge_density > 0.15 and small_contours > 50:
            return "text"  # å¯èƒ½æ˜¯è—¥å–®/æ–‡ä»¶
        elif len(contours) < 10 and edge_density < 0.1:
            return "object"  # å¯èƒ½æ˜¯å–®ä¸€è—¥ç‰©
        else:
            return "mixed"  # ä¸ç¢ºå®š
    except Exception as e:
        print(f"åœ–ç‰‡é¡å‹åˆ¤æ–·å¤±æ•—: {e}")
        return "mixed"


if __name__ == "__main__":
    test_recognition()
